[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I have more than 10 years of experience in the oil & gas exploration industry and I have created Geophysics Labs in March 2016 to develop my ideas for new and original interpretation tools. I am also an experienced researcher and this gives me a unique perspective on both the industrial and the academic worlds. Since my PhD in 2001 at the University of Lyon (France), I have regularly published my work in renowned academic journals, as well as in more industry-related publications.\nI have accumulated a wealth of experience on all the various aspects of potential field interpretation: structural interpretation, 2D/3D modelling and integration with seismic data. I worked as lead interpreter for many proprietary or multi-client studies, regional or local, in various tectonic settings:\n\nfold-and-thrust belts (USA, Algeria, Turkey)\npassive margins (North Atlantic, West Africa)\nsalt basins (Gabon)\nrift systems (Kenya, Uganda)\n\nI am a specialist of gravity gradiometry, having worked with full-tensor gradiometer (FTG) data for 9 years. Using a new technology like the FTG gave me the incentive for creating innovative tools that could utilise the full potential of the technique. I am focusing my R&D effort on the improvements of quantitative interpretation methods and on the integration of their results with seismic data."
  },
  {
    "objectID": "about.html#dr-joseph-barraud",
    "href": "about.html#dr-joseph-barraud",
    "title": "About",
    "section": "",
    "text": "I have more than 10 years of experience in the oil & gas exploration industry and I have created Geophysics Labs in March 2016 to develop my ideas for new and original interpretation tools. I am also an experienced researcher and this gives me a unique perspective on both the industrial and the academic worlds. Since my PhD in 2001 at the University of Lyon (France), I have regularly published my work in renowned academic journals, as well as in more industry-related publications.\nI have accumulated a wealth of experience on all the various aspects of potential field interpretation: structural interpretation, 2D/3D modelling and integration with seismic data. I worked as lead interpreter for many proprietary or multi-client studies, regional or local, in various tectonic settings:\n\nfold-and-thrust belts (USA, Algeria, Turkey)\npassive margins (North Atlantic, West Africa)\nsalt basins (Gabon)\nrift systems (Kenya, Uganda)\n\nI am a specialist of gravity gradiometry, having worked with full-tensor gradiometer (FTG) data for 9 years. Using a new technology like the FTG gave me the incentive for creating innovative tools that could utilise the full potential of the technique. I am focusing my R&D effort on the improvements of quantitative interpretation methods and on the integration of their results with seismic data."
  },
  {
    "objectID": "legacy/create-globes-with-basemap-and-cartopy.html",
    "href": "legacy/create-globes-with-basemap-and-cartopy.html",
    "title": "Create Globes with Basemap and Cartopy",
    "section": "",
    "text": "In this short post, I am showing how to create maps in the orthographic projection, which, to most of us, corresponds to the globe as seen from outer space. As usual, the code and some additional examples are available in a jupyter notebook in the interpies repository.\n\nPython libraries for plotting 2D data on maps\nThe two libraries I use here are basemap and cartopy. The first one is no longer in development but is still a perfectly valid choice for our purpose here. Cartopy is actually going to officially replace Basemap at some point, so it makes sense to look at the equivalent solution for creating globes.\nAnother solution would be to use the newly developed Python interface for GMT (GMT/Python), which looks promising. But that might be for another post…\n\n\nMake a globe\nBoth libraries work in principle in a similar way: first create a map object with the projection of your choice and then add features like vector layers (i.e., coastlines) and images to it. In basemap, setting up a map is done with the Basemap class.\nfrom mpl_toolkits.basemap import Basemap\nm = Basemap(projection='ortho', lon_0=0, lat_0=45, resolution='c')\n\n# draw coastlines and borders\nm.drawcoastlines()\nm.drawcountries()\n\n# draw meridians and parallels\nm.drawmeridians(np.arange(0, 360, 30))\nm.drawparallels(np.arange(-90, 90, 30))\nplt.show()\n\n\n\nThe Earth in an orthographic projection (Basemap)\n\n\nIn cartopy, things are actually closer to the typical matplotlib philosophy: create Axes with plt.axes and tell matplotlib to use a specific projection (Axes then become GeoAxes). Then use methods to draw objects within the axes.\nimport cartopy\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nax = plt.axes(projection=ccrs.Orthographic(0, 45))\n\n# draw coastlines and borders\nax.add_feature(cfeature.COASTLINE)\nax.add_feature(cfeature.BORDERS, lw=0.5)\n\n# draw meridians and parallels\ngl = ax.gridlines(color='k', linestyle=(0, (1, 1)),\n                  xlocs=range(0, 390, 30),\n                  ylocs=[-80, -60, -30, 0, 30, 60, 80])\n\n\n\nThe Earth in an orthographic projection (cartopy)\n\n\n\n\nGlobal Gravity Data\nThe satellite gravity anomalies from Sandwell are perfect for this exercise (Sandwell et al., 2014). However, it is not a truly global dataset as it actually does not cover the poles. That’s not an issue for cartopy. However, a few more steps will be necessary in basemap to achieve the same result.\nI explained in a previous post how to create a plain grid of Sandwell’s gravity anomalies. Using interpies I can then load the grid and create an image with hillshading and a colormap applied.\n\n\n\nFree-air gravity anomalies derived from satellite altimetry (“Sandwell and Smith” data version 24.1, data: SIO, NOAA, NGA)\n\n\nIn interpies, I have recently added a new method to save as an image the map without the labels and the colorbar.\nimport interpies\ngrid1 = interpies.open('../data/sandwell_grav_v24_10min.tif')\ngrid1.save_image('../data/grav_v24_10min.png',\n                 cmap_brightness=1.5, hs_contrast=3.0)\nWarping the resulting image around the globe with cartopy is then as easy as using the imshow method in matplotlib. The fact that higher latitudes are not covered is dealt with by setting the correct extent.\nim = imageio.imread('../data/grav_v24_10min.png')\nax = plt.axes(projection=ccrs.Orthographic(0, 45))\nax.imshow(im[:,:,:3], extent=[0.001, 360, -80.720, 80.738],\n          origin='upper', transform=ccrs.PlateCarree())\n\n# draw coastlines and borders\nax.add_feature(cfeature.COASTLINE)\nax.add_feature(cfeature.BORDERS, lw=0.5)\n\n# draw meridians and parallels\ngl = ax.gridlines(color='k', linestyle=(0, (1, 1)),\n                  xlocs=range(0, 390, 30),\n                  ylocs=[-80, -60, -30, 0, 30, 60, 80])\n\n\n\nGravity anomalies in an orthographic projection (cartopy, data: SIO, NOAA, NGA)\n\n\nIn basemap, things are a little more complicated because the warping function accepts only images that cover the whole globe and that start at -180 degrees. So our initial image needs to be both padded with white borders and shifted by 180 degrees. See the notebook on GitHub for the details about the way to achieve this.\n\n\nReferences\nSandwell, D. T., R. D. Müller, W. H. F. Smith, E. Garcia, R. Francis. 2014. New global marine gravity model from CryoSat-2 and Jason-1 reveals buried tectonic structure, Science, Vol. 346, no. 6205, pp. 65-67, doi: 10.1126/science.1258213."
  },
  {
    "objectID": "legacy/hillshading-with-matplotlib.html",
    "href": "legacy/hillshading-with-matplotlib.html",
    "title": "Hillshading with matplotlib",
    "section": "",
    "text": "Hillshading simulates the variable illumination of a surface by a directional light source. It is a great method to represent relief on a map and it works very well with potential field data too, not only with topographic data. Creating shaded maps in Python with matplotlib is easy and a few examples are provided here. However, a simple transparency blend of the hillshade with the coloured data seems to be missing. So I have added this option to a modified version of imshow."
  },
  {
    "objectID": "legacy/hillshading-with-matplotlib.html#summary",
    "href": "legacy/hillshading-with-matplotlib.html#summary",
    "title": "Hillshading with matplotlib",
    "section": "",
    "text": "Hillshading simulates the variable illumination of a surface by a directional light source. It is a great method to represent relief on a map and it works very well with potential field data too, not only with topographic data. Creating shaded maps in Python with matplotlib is easy and a few examples are provided here. However, a simple transparency blend of the hillshade with the coloured data seems to be missing. So I have added this option to a modified version of imshow."
  },
  {
    "objectID": "legacy/hillshading-with-matplotlib.html#introduction",
    "href": "legacy/hillshading-with-matplotlib.html#introduction",
    "title": "Hillshading with matplotlib",
    "section": "Introduction",
    "text": "Introduction\nShaded relief has been used to represent topography on maps for centuries. Adding a sense of visual relief by drawing shades according to the direction of the sun is an art when it is done manually like cartographers used to do before the advances of mapping software. Today, hillshading is mostly generated analytically from digital elevation models. Even so, professionals still consider that the best results are obtained when manual adjustments are made to emphasize conceptually important terrain features like ridge lines.\nAs it is one of the best ways to highlight small details in gridded data, hillshading is also used for all sorts of data, especially gravity and magnetic data. It can even be used to display seismic data in a novel way.\nI have created the figures in this post with a modified version of imshow, a function of matplotlib.pyplot that displays 2D arrays. The modification adds options like contours, colorbar and hillshading directly as standard features of the plot. A previous post introduced parameters related to colormaps."
  },
  {
    "objectID": "legacy/hillshading-with-matplotlib.html#parameters",
    "href": "legacy/hillshading-with-matplotlib.html#parameters",
    "title": "Hillshading with matplotlib",
    "section": "Parameters",
    "text": "Parameters\n\nAzimuth and altitude\nSeveral parameters control the shading effect when the illumination is calculated analytically. The azimuth and altitude of the light source are essential parameters. Although the sun is typically placed in the northwest corner for topographic maps, the convention is not as strict for geophysical data because changing the azimuth can be a useful way to highlight various structural directions within the data.\nHere is an example with some magnetic data downloaded from the USGS website. The aeromagnetic survey covers an area of New Mexico in the US. The images below show the magnetic anomalies displayed as a shaded surface for two different light directions.\n\n\n\n\n\n\n\n\n\n\n\n(a) Hillshading is generated with a light source in the northwest corner.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Hillshading is generated with a light source in the northeast corner.\n\n\n\n\n\n\n\nFigure 1: Magnetic anomalies in an area of New Mexico.\n\n\n\n\n\nVertical exaggeration\nThe other essential parameter for adjusting the intensity of the shading is the vertical exaggeration, commonly also called Z factor. This is a coefficient applied to vertical distances and its effect is to rescale the surface heights relative to the horizontal dimensions. While it might be preferable to keep this number close to one in the case of topographic data, it may actually be necessary to use much larger values when the range of the data is small, for example with gravity anomalies.\nHere is an example with Bouguer anomalies from a recent survey in the offshore Rockall basin, west of Scotland.\n\n\n\n\n\n\nFigure 2: Bouguer anomalies at the northeastern end of the Rockall Basin. Vertical exaggeration is equal to 1 on the left and 1000 on the right."
  },
  {
    "objectID": "legacy/hillshading-with-matplotlib.html#blending",
    "href": "legacy/hillshading-with-matplotlib.html#blending",
    "title": "Hillshading with matplotlib",
    "section": "Blending",
    "text": "Blending\nThe shaded image on its own is not necessarily interesting and so it is generally combined with a coloured rendering of the data. Blending the colormapped data with the grayscale image of the shaded relief can be done in various ways and three options are available in matplotlib via the LightSource class: hsv, overlay, and soft.\nHere is a demonstration inspired from the matplotlib example for hillshading.\n\n\n\n\n\n\nFigure 3: Topographic data combined with hillshading using three different blending modes (from left to right): hsv, overlay and soft. The colormap is gist_earth.\n\n\n\nThe hsv mode does not look very realistic and has this ‘plastic’ feel about it. The other two modes are similar, the overlay mode showing the largest contrast.\nLet’s see another example with a different colormap, ‘clra’ from Geosoft.\n\n\n\n\n\n\nFigure 4: Topographic data combined with hillshading using three different blending modes (from left to right): hsv, overlay and soft. The colormap is clra from Geosoft.\n\n\n\nWeirdly, both the overlay and soft modes look over-saturated and too bright. The hillshading is actually barely visible. I am not sure what is causing this “reaction” of the blending to this particular colormap. I noticed a similar effect with ‘jet’, ‘seismic’ and ‘spectral’.\nThis test prompted me to work on a new blending mode: alpha blending. This is a simple linear combination of the RGB coloured image of the data with the grayscale intensity of the hillshade.\nblend = alpha*rgb + (1 - alpha)*intensity\nThe alpha parameter controls the amount of transparency of the coloured image, 1 being completely opaque and 0 completely transparent.\nThe resulting effect for alpha = 0.7 and for a variety of colormaps is shown below.\n\n\n\n\n\n\n\n\n\n\n\n(a) Coolwarm colormap.\n\n\n\n\n\n\n\n\n\n\n\n(b) Geosoft colormap.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Gist_earth colormap.\n\n\n\n\n\n\n\n\n\n\n\n(d) Jet colormap.\n\n\n\n\n\n\n\nFigure 5: Topographic data displayed with hillshading and alpha blending."
  },
  {
    "objectID": "legacy/hillshading-with-matplotlib.html#python-implementation",
    "href": "legacy/hillshading-with-matplotlib.html#python-implementation",
    "title": "Hillshading with matplotlib",
    "section": "Python implementation",
    "text": "Python implementation\nThe hillshade and new blending mode are available in a function called imshow_hs that I have written in Python. The function is part of a small package called graphics. It obviously requires matplotlib to work.\nThe easiest way to learn about the various options and parameters of imshow_hs is to look at the Jupyter notebook that is available on the GitHub repository."
  },
  {
    "objectID": "legacy/histogram-equalization-in-python-and-matplotlib.html",
    "href": "legacy/histogram-equalization-in-python-and-matplotlib.html",
    "title": "Histogram Equalization in Python and matplotlib",
    "section": "",
    "text": "When displaying geophysical data in a map, one may find it difficult to show both the presence of extremes and the subtle variations in the background signal. Histogram equalization is there to help, as it redistributes intensities and increases the contrast. In this new implementation for Python and matplotlib, the equalization is applied  to the colormap rather than the data. This allows the user to show the real distribution of intensities on the colorbar. Other ways to improve the visualisation of anomalies and their amplitude are also presented."
  },
  {
    "objectID": "legacy/histogram-equalization-in-python-and-matplotlib.html#summary",
    "href": "legacy/histogram-equalization-in-python-and-matplotlib.html#summary",
    "title": "Histogram Equalization in Python and matplotlib",
    "section": "",
    "text": "When displaying geophysical data in a map, one may find it difficult to show both the presence of extremes and the subtle variations in the background signal. Histogram equalization is there to help, as it redistributes intensities and increases the contrast. In this new implementation for Python and matplotlib, the equalization is applied  to the colormap rather than the data. This allows the user to show the real distribution of intensities on the colorbar. Other ways to improve the visualisation of anomalies and their amplitude are also presented."
  },
  {
    "objectID": "legacy/histogram-equalization-in-python-and-matplotlib.html#introduction",
    "href": "legacy/histogram-equalization-in-python-and-matplotlib.html#introduction",
    "title": "Histogram Equalization in Python and matplotlib",
    "section": "Introduction",
    "text": "Introduction\nGeophysical data typically contain a huge range of values. This is not surprising: geophysicists are interested in anomalies, what is different from the average, i.e. abnormal. So for example, searching for ores or metals underground, one might use a magnetometer to find the presence of magnetic sources. The objective is in this case to locate what is magnetic against what is not, i.e. to find a coherent signal against the noisy background of “normal” rocks.\nBut sometimes the objective is different: the geophysicist might need to highlight small changes in the background field, and in this case the large anomalies are disturbing the interpretation. This might happen when surveying for deep sources: their weak signal will be dwarfed by the large “noise” produced by shallow features.\nVisualising data can be a tricky business and a lot has been written or said on the subject. It all depends on what you are trying to demonstrate and the proper use of visualisation techniques can help with your goal. But some techniques might also introduce unreal features, or deceive the reader about the true intensity of the recorded anomalies.\nIn this post, I am focusing on colormaps and I present histogram equalization as a way to enhance the display of geophysical data in maps. Its effect is compared to the standard linear scaling function. The implementation has been written in Python and makes use of the matplotlib library."
  },
  {
    "objectID": "legacy/histogram-equalization-in-python-and-matplotlib.html#scaling-and-the-choice-of-color-map",
    "href": "legacy/histogram-equalization-in-python-and-matplotlib.html#scaling-and-the-choice-of-color-map",
    "title": "Histogram Equalization in Python and matplotlib",
    "section": "Scaling and the choice of color map",
    "text": "Scaling and the choice of color map\nIn the process of displaying gridded data with pseudo-colours, there are two important aspects: the scaling function (also called normalisation, or stretch, or classification) and the color map. A lot of authors have been concerned in the last 10 years or so about the use of the rainbow colormap (called “jet” in MATLAB and matplotlib). New, more efficient and more rational colormaps have been proposed and some of them have now become the new default choice in both MATLAB (“parula”) and matplotlib (“viridis”).\n\n\n\nFour colour maps.\n\n\nWhile this article focuses on the first part of the mapping process (scaling), the effect obviously depends a lot on the colormap in use. I use four colormaps here: jet, viridis, coolwarm and clra (geosoft). The first three are available in matplotlib, the fourth one is the default colormap in Geosoft Oasis Montaj, and is somehow considered as the standard choice for potential field applications (although this might change in the future as perceptually uniform colormaps become more prominent).\n\n\n\nLightness L* profile for four different colormaps: jet, viridis, coolwarm and geosoft clra\n\n\nThere are various ways to compare the qualities of colormaps (and again, this is not a post about colormaps) and one of the simplest is to calculate the lightness L* after conversion of the colours from the RGB to the CIELAB colorspace.\nAs seen on the L* profiles, the jet and clra (geosoft) colormaps show bumpy profiles with a few isoluminant portions (nearly constant lightness), which are one of the sources of criticism against this kind of rainbow-like colormaps. Thus, with such palettes, the observer would perceive variations only at colour boundaries (Borland and Talor, 2007).\nIn contrast, viridis and coolwarm have been designed to show simple uniform variations in lightness, which are therefore supposed to match the underlying variations of the mapped quantity, improving the perception, or interpretation of the map (Moreland, 2009).\nThis last point naturally leads me to the object of this post: a map is a representation of the data and it must serve the purpose of the interpreter. If the scaling function is a linear interpolation (simply matching the minimum and maximum scalar values to the colours at the extremities of the palette), then the perceptually uniform colormap has achieved its goal: the observer can read the map directly to estimate the intensity of the field (or whatever quantity is displayed). However, this might not always be the best way to convey to the reader the entire content of the data. It actually depends on the story you want to tell.\nLet’s see how this works in practice with magnetic data, in an area where large variations in anomalies have been recorded. This example comes from an aeromagnetic survey acquired in New Mexico for the USGS.\n\n\n\nHistogram of magnetic data from the South Silver City survey (New Mexico).\n\n\nTypically for magnetic data, the minimum (-1850 nT) and the maximum (2220 nT) values are way outside the bulk of the data, which shows roughly a gaussian distribution, as shown on the histogram. This means that about 95% of the intensities are in the interval [mean - 2*sigma, mean + 2*sigma], which is about [-240,362].\nUsing the default options of the imshow() function of the matplotlib.pyplot module, the following map of the magnetic anomalies can be obtained.\n\n\n\nMagnetic anomalies in an area in New Mexico. The “jet” rainbow colormap is used together with a linear scale (no stretch) and a standard colorbar.\n\n\nIf the objective of the map was simply to locate the largest anomalies, then it is not so bad. The two or three dark blue and red blobs in southern central region of the survey are well visible against the greenish background. However, looking for additional information in this nearly uniform background, the bright yellow and cyan patches compete to drag our attention. It is confusing.\nLet’s see how viridis performs:\n\n\n\nMagnetic anomalies in an area in New Mexico. The viridis colormap is used together with a linear scale (no stretch) and a standard colorbar.\n\n\nThe minima and maxima are still clearly identified and this time the rest of the map contains brighter yellow spots that are easier to differentiate against the blue-green background that is supposed to correspond to the “absence” of magnetic material (zero magnetic anomaly). That’s better."
  },
  {
    "objectID": "legacy/histogram-equalization-in-python-and-matplotlib.html#the-power-of-the-colorbar",
    "href": "legacy/histogram-equalization-in-python-and-matplotlib.html#the-power-of-the-colorbar",
    "title": "Histogram Equalization in Python and matplotlib",
    "section": "The power of the colorbar",
    "text": "The power of the colorbar\nThe maps in this post have actually been made using my own modified version of the imshow function. The idea is to have access in a single tool to a variety of functions I often use together with imshow. I have also created additional options and I am going to document them in this post and the next ones.\nOne of the simplest ways to improve the existing map is to change the labels on the colorbar. Instead of showing equally spaced numbers like on the previous plots, I think showing basic descriptive statistics on the colorbar is more informative and useful. This is how it looks:\n\n\n\nMagnetic anomalies in an area in New Mexico. The viridis colormap is used together with a linear scale (no stretch) and a modified colorbar that indicates basic descriptive statistics.\n\n\nSo now the labels on the colorbar indicate, from top to bottom: max, mean+2*sigma, mean, mean-2*sigma, and min (sigma being the standard deviation). Since a map is rarely given with the histogram of the data it contains, putting statistical information on the colorbar makes it easier for the user to understand the distribution of the data.\nIn our magnetic case, this simply confirms the impression the reader must have when seeing this mostly blue-green image: most of the data is “compressed” in a narrow range of values.\nHere are two other examples with the coolwarm and geosoft clra colormaps.\n\n\n\nMagnetic anomalies in an area in New Mexico. The coolwarm colormap is used together with a linear scale (no stretch) and a modified colorbar that indicates basic descriptive statistics.\n\n\n\n\n\nMagnetic anomalies in an area in New Mexico. The geosoft colormap is used together with a linear scale (no stretch) and a modified colorbar that indicates basic descriptive statistics.\n\n\nThe geosoft colormap is emphasising thanks to multiple colour shifts the presence of small variations around the mean of the data, something jet could not do because of the long stretch of cyan and green colours in the middle of the colormap. The addition of the bright pink at the top of the geosoft colormap also helps separating the extremes from the average. This explains why this colormap has been quite popular for mapping potential field data."
  },
  {
    "objectID": "legacy/histogram-equalization-in-python-and-matplotlib.html#histogram-equalization",
    "href": "legacy/histogram-equalization-in-python-and-matplotlib.html#histogram-equalization",
    "title": "Histogram Equalization in Python and matplotlib",
    "section": "Histogram equalization",
    "text": "Histogram equalization\nHistogram equalization (or simply equalization) has been used to increase the contrast of images for a long time. It was particularly useful to improve early satellite images that could look a bit dull through the haze of the atmosphere.\nEqualization works by spreading out intensity values more evenly. The transformation aims at flattening the histogram, rebalancing the intensities over the whole span of colours (or shades of grey). Here is the equalized histogram of the magnetic data we have seen earlier.\n\n\n\nEqualized histogram of magnetic data from the South Silver City survey (New Mexico).\n\n\nThere are two ways to implement histogram equalization, either as an image change (like in scikit-image), or as a colormap change. I have chosen the second option as it has two advantages: the data remain untouched and the new colorbar clearly shows the distortion applied to the colormap.\nThe effect of histogram equalization depends on the input data so the new colormap is unique and cannot be re-used for a different data set.\nHere is an example with the geosoft clra colormap.\n\n\n\nMagnetic anomalies in an area in New Mexico. The geosoft clra colormap is used together with histogram equalization and a modified colorbar that indicates basic descriptive statistics.\n\n\nThe result might look dramatic but that is intended. Minute variations in the data are now revealed and some sharp lineaments are immediately visible. This new map makes the structural interpretation of the magnetic data easier because for this purpose I am more interested in spatial correlations and the shape of features than in their intensity. I also see all the tiny anomalies, as if the instrument was suddenly much more sensitive.\nThe application and effects of such severe contrast enhancement techniques should not be concealed or minimized. The new colorbar makes it obvious that large portions of the data are displayed with pretty much the same colour (pink or blue). The idea of histogram equalization is not to pretend that the data is much better than it is in reality. The change is only misleading if the information about it is not provided clearly and honestly, in the legend or in the caption."
  },
  {
    "objectID": "legacy/histogram-equalization-in-python-and-matplotlib.html#contours",
    "href": "legacy/histogram-equalization-in-python-and-matplotlib.html#contours",
    "title": "Histogram Equalization in Python and matplotlib",
    "section": "Contours",
    "text": "Contours\nAnother way to remind the reader of the map that some of the anomalies are much larger than the others is to add contours. Contours are drawn at regular intervals so one obtains a map that combines the best of both worlds: the enhanced contrast of the non-linear colormap and the sense of scale offered by the evenly-spaced contour lines.\nWith matplotlib, it is even possible to also have the contour lines on the colorbar! Here is an example with the coolwarm colormap.\n\n\n\nMagnetic anomalies in an area in New Mexico. The map illustrates the use of histogram equalization with the coolwarm colormap. The addition of contours at regular intervals provides a linear scale to help estimating the intensity of the anomalies."
  },
  {
    "objectID": "legacy/histogram-equalization-in-python-and-matplotlib.html#python-implementation",
    "href": "legacy/histogram-equalization-in-python-and-matplotlib.html#python-implementation",
    "title": "Histogram Equalization in Python and matplotlib",
    "section": "Python implementation",
    "text": "Python implementation\nI have implemented histogram equalization in a Python module called graphics. It contains two modules:\n\ncolors: this contains the definition of new colormaps for matplotlib (essentially clra and clrb from Geosoft Oasis Montaj).\ngraphics: a collection of functions for manipulating and displaying grid data.\n\nThe main function in graphics is imshow_hs, which is my modified version of pyplot.imshow. It offers a lot of parameters, some of them are simply imported from other pyplot functions (like colorbar and contours), and some of them are new (equalization and improved hillshading). See the documentation in the code on GitHub for more information. I have also prepared a Jupyter notebook that goes through some of the main options.\nThe hillshading option will be the subject of another post."
  },
  {
    "objectID": "legacy/histogram-equalization-in-python-and-matplotlib.html#conclusion",
    "href": "legacy/histogram-equalization-in-python-and-matplotlib.html#conclusion",
    "title": "Histogram Equalization in Python and matplotlib",
    "section": "Conclusion",
    "text": "Conclusion\nI have added histogram equalization to a modified version of matplotlib’s imshow. This is a quick and easy way to boost the contrast of images and it helps visualizing the full content of geophysical data.\nManipulating data in this way might sound suspicious since the current evolution of scientific visualization software is to promote the unbiased, neutral display of data with the use of perceptually uniform colormaps. However, pseudocoloring is not the only way to communicate about the amplitudes and variations of data: descriptive statistics, hillshading, contours and 2D profiles are also quite efficient, if not better.\nThere are lots of methods to improve the “look” of geophysical data and geophysicists are good at inventing all sorts of them: rescaling, clipping, filtering or applying derivatives can all contribute to an enhanced picture that is easier to interpret.\nThe important thing is to be honest and clear about the tricks that have been applied to create that pretty picture. This is why colorbars and figure captions are important!"
  },
  {
    "objectID": "legacy/histogram-equalization-in-python-and-matplotlib.html#links",
    "href": "legacy/histogram-equalization-in-python-and-matplotlib.html#links",
    "title": "Histogram Equalization in Python and matplotlib",
    "section": "Links",
    "text": "Links\n\nData\nUSGS magnetic data: South Silver City (4017) Magnetic Anomaly Map\n\n\nWikipedia\nhttps://en.wikipedia.org/wiki/Histogram_equalization\nhttps://en.wikipedia.org/wiki/Exploration_geophysics\nhttps://en.wikipedia.org/wiki/Lab_color_space\n\n\nTED Talks\nhttp://www.ted.com/talks/david_mccandless_the_beauty_of_data_visualization http://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen\n\n\nBlogs and articles\nMatplotlib doc: Choosing Colormaps\nMyCarta: The rainbow is dead…long live the rainbow!\nKenneth Moreland: Diverging Color Maps for Scientific Visualization\nBorland, David, and Russell M. Taylor Ii. “Rainbow Color Map (Still) Considered Harmful.” IEEE Computer Graphics and Applications 27.2 (2007): 14-17. DOI: 10.1109/MCG.2007.323435"
  },
  {
    "objectID": "legacy/how-to-add-cross-sections-to-opendtect.html",
    "href": "legacy/how-to-add-cross-sections-to-opendtect.html",
    "title": "How to add cross-sections to OpendTect?",
    "section": "",
    "text": "Note\n\n\n\nThis post is associated with a Jupyter notebook available on GitHub.\n\n\nIn a previous post, I explained how to add colour maps to the 3D environment of OpendTect. The method is simply to convert the RGB image of the map into an indexed colour image. The resulting grid can be loaded as any other horizon and the colours are provided by a fixed palette.\nIn this post, I will show how to apply the same method to insert images of cross-sections to OpendTect. This is in essence a conversion of an image into seismic data."
  },
  {
    "objectID": "legacy/how-to-add-cross-sections-to-opendtect.html#introduction",
    "href": "legacy/how-to-add-cross-sections-to-opendtect.html#introduction",
    "title": "How to add cross-sections to OpendTect?",
    "section": "",
    "text": "Note\n\n\n\nThis post is associated with a Jupyter notebook available on GitHub.\n\n\nIn a previous post, I explained how to add colour maps to the 3D environment of OpendTect. The method is simply to convert the RGB image of the map into an indexed colour image. The resulting grid can be loaded as any other horizon and the colours are provided by a fixed palette.\nIn this post, I will show how to apply the same method to insert images of cross-sections to OpendTect. This is in essence a conversion of an image into seismic data."
  },
  {
    "objectID": "legacy/how-to-add-cross-sections-to-opendtect.html#method",
    "href": "legacy/how-to-add-cross-sections-to-opendtect.html#method",
    "title": "How to add cross-sections to OpendTect?",
    "section": "Method",
    "text": "Method\nThe basic steps of the method can be described as follows:\n\nPrepare the RGB image of the cross-section.\nResize and resample the image to fit the appropriate dimensions of the section.\nConvert the image from RGB to indexed colour using colour quantization\nImport the result into OpendTect as seismic data.\n\nWhile this seems straightforward, there can be several issues on the way. Problems are mostly related to the proper positioning of the cross-section in space.\nOne relatively important point to note is that resampling should ideally happen before colour quantization. This is because resampling might modify the colour index, which is an integer. Of course, one way to avoid this problem is to use nearest-neighbour interpolation during resampling (see the previous post)."
  },
  {
    "objectID": "legacy/how-to-add-cross-sections-to-opendtect.html#example-the-kevitsa-deposit-northern-finland",
    "href": "legacy/how-to-add-cross-sections-to-opendtect.html#example-the-kevitsa-deposit-northern-finland",
    "title": "How to add cross-sections to OpendTect?",
    "section": "Example: the Kevitsa Deposit, northern Finland",
    "text": "Example: the Kevitsa Deposit, northern Finland\nIn order to describe the method more efficiently, I am using an example based on data that have recently been made freely available thanks to the Frank Arnott award. It is a complete geophysical dataset that has been used for the exploration of the Kevitsa intrusion in Finland (Malehmir et al., 2012). It contains a 3D reflection seismic survey, potential-field survey data, wells, geological maps (Figure 1) and cross-sections.\n\n\n\n\n\n\nFigure 1: Geological map of the Kevitsa area. The 3D seismic survey area is shown in red and the Titan cross-section in black and green (after Koivisto et al., 2015).\n\n\n\nFor this demonstration, I am using a cross-section called Titan Line E5 that runs across the Kevitsa mafic-ultramafic intrusion (Figure 1). The section was produced by interpretation and inversion of magnetotelluric (MT) and electromagnetic measurements that were collected by Quantec Geoscience Ltd during a survey in 2008. The interpretation, performed by First Quantum Minerals Ltd, shows the shape of the main olivine pyroxenite intrusion, the presence of smaller dunite bodies, and the surrounding sedimentary and volcanic rocks (Figure 2).\n\n\n\n\n\n\nFigure 2: Titan Line E5: cross-section of the Kevitsa ultramafic intrusion and surrounding rocks. Courtesy of First Quantum Minerals Ltd."
  },
  {
    "objectID": "legacy/how-to-add-cross-sections-to-opendtect.html#resizing-and-resampling",
    "href": "legacy/how-to-add-cross-sections-to-opendtect.html#resizing-and-resampling",
    "title": "How to add cross-sections to OpendTect?",
    "section": "Resizing and Resampling",
    "text": "Resizing and Resampling\nOne of the difficulties with importing images of cross-sections is the presence of annotations, text and other grids. It is therefore important to start the process with a clean image that has been stripped of those disturbing elements. When importing from a PDF file, a program like Inkscape can be very useful to extract only the essential information.\nThe cross-section is georeferenced (generally) by its start and end points, both in the horizontal and vertical dimensions. So a tight crop of the image to these reference points is the next step. Sometimes a gain in precision can be achieved by removing bits on the sides. For example, in my example of the Titan Line E5, I cropped the bottom of the image to a depth of 1500 m instead of trying to estimate the actual depth of the deepest point of the section.\nOnce a good quality image of the section is obtained, it might be necessary to resample it to match the typical dimensions of a seismic line in the project you are working on. “Pixels” in seismic data are rarely square, i.e. the spaces between traces and samples in depth-converted data are not equal. In effect, the seismic “pixel” is stretched in the horizontal dimension. This ensures a good resolution in the vertical dimension, especially as most people would actually apply a large vertical exaggeration when doing any interpretative work. So to replicate this in our case, we need to oversample the image in the vertical dimension.\nFor the Titan line, I have increased the number of pixels in the Z dimension (the number of rows) from 704 to 1000 (Figure 3). Details about the Python implementation can be found in this Jupyter notebook.\n\n\n\n\n\n\nFigure 3: RGB image of the Titan E5 cross-section after cropping and resizing. The numbers on the axes are row and column indices."
  },
  {
    "objectID": "legacy/how-to-add-cross-sections-to-opendtect.html#colour-quantisation",
    "href": "legacy/how-to-add-cross-sections-to-opendtect.html#colour-quantisation",
    "title": "How to add cross-sections to OpendTect?",
    "section": "Colour Quantisation",
    "text": "Colour Quantisation\nThis stage in the process is very similar to what was done in the case of maps. I use scikit-learn and a pairwise distance function to match the colours in the image with one of 256 colours of a fixed palette (the 8-bit Windows palette).\nSince the cross-section makes use of simple coloured patches, the quantised result looks almost identical to the original RGB picture (Figure 4). The advantage of course is that there is only one band instead of three.\n\n\n\n\n\n\nFigure 4: Image of the Titan E5 cross-section after quantisation. The numbers on the axes are row and column indices."
  },
  {
    "objectID": "legacy/how-to-add-cross-sections-to-opendtect.html#conversion-to-segy",
    "href": "legacy/how-to-add-cross-sections-to-opendtect.html#conversion-to-segy",
    "title": "How to add cross-sections to OpendTect?",
    "section": "Conversion to SEGY",
    "text": "Conversion to SEGY\nThe quantised image now needs to be converted to a format that OpendTect can understand. While the simplest solution might be to use a text file in an ASCII format (“Simple File” in OpendTect jargon), I am converting the section to the SEGY format as this makes the method more generic. This also gives me the opportunity to use the obspy library!\nReferring to the final cells of the notebook, here is a breakdown of this stage:\n\nUsing the coordinates of the start and end points of the line, a bit of Python code is used to calculate the coordinates of each trace (each column in the image) by linearly interpolating between the two extremities.\nThe next step is to define some parameters like the coordinate scaling factor and the sample interval. The scaling factor makes it possible to store coordinates at centimetre-scale precision using only integers.\nOptionally, a text header can be added to the file to keep some information about the data.\nThe final step is to write all this information to file."
  },
  {
    "objectID": "legacy/how-to-add-cross-sections-to-opendtect.html#importing-the-section-in-opendtect",
    "href": "legacy/how-to-add-cross-sections-to-opendtect.html#importing-the-section-in-opendtect",
    "title": "How to add cross-sections to OpendTect?",
    "section": "Importing the section in OpendTect",
    "text": "Importing the section in OpendTect\nWhile loading the resulting SEGY file into OpendTect should be straightforward and identical to importing “normal” seismic data, it is essential to also add the 8-bit palette of 256 colours that were used for the quantisation. I refer you to the end of the previous post for the instructions on how to do that.\n\n\n\n\n\n\nFigure 5: View of the Titan E5 Line in 3D in OpendTect, together with depth-converted seismic data."
  },
  {
    "objectID": "legacy/how-to-add-cross-sections-to-opendtect.html#conclusion",
    "href": "legacy/how-to-add-cross-sections-to-opendtect.html#conclusion",
    "title": "How to add cross-sections to OpendTect?",
    "section": "Conclusion",
    "text": "Conclusion\nHaving the cross-section side-by-side with the reflection seismic data and the geological map in a 3D environment helps to appreciate the differences and similarities between these three types of information (Figure 5). The dataset could be completed to include wells, magnetic data, other cross-sections, etc. It would then become the ideal tool for exploration in this part of the world."
  },
  {
    "objectID": "legacy/how-to-add-cross-sections-to-opendtect.html#references",
    "href": "legacy/how-to-add-cross-sections-to-opendtect.html#references",
    "title": "How to add cross-sections to OpendTect?",
    "section": "References",
    "text": "References\nKoivisto, E., Malehmir, A., Hellqvist, N., Voipio, T., Wijns, C., 2015. Building a 3D model of lithological contacts and near-mine structures in the Kevitsa mining and exploration site, Northern Finland: Constraints from 2D and 3D reflection seismic data. Geophysical Prospecting 63, 754–773. doi:10.1111/1365-2478.12252\nMalehmir, A., Juhlin, C., Wijns, C., Urosevic, M., Valasti, P., Koivisto, E., 2012. 3D reflection seismic imaging for open-pit mine planning and deep exploration in the Kevitsa Ni-Cu-PGE deposit, northern Finland. Geophysics 77, WC95-WC108. doi:10.1190/geo2011-0468.1"
  },
  {
    "objectID": "legacy/three-notebooks-to-jump-start-a-data-science-project.html",
    "href": "legacy/three-notebooks-to-jump-start-a-data-science-project.html",
    "title": "Three notebooks to jump start a data science project",
    "section": "",
    "text": "I was in Paris last week for the 2017 Subsurface Hackathon. It was superbly organised by Agile Scientific and Total. Together, they managed to draw a large (about 60 persons) and diverse crowd to participate to the challenge of producing some working and usable software in only two days. The event took place over the weekend just before the EAGE conference, so you could tell people were highly motivated to come and share the experience.\nI was with Martin Bentley in a small team called Water Underground. While Martin was actually sitting in Port Elizabeth in South Africa, I was in La Defense in Total’s brand new offices. Our idea was to mine some groundwater data from the Geological Survey of the Netherlands and use the measurements to predict the evolution of water depth and composition both in space and time. Unfortunately, we did not manage to complete the project. As any data scientist would tell you, the first (and somewhat less rewarding) steps of any data science project can take a lot of time, generally more than expected…"
  },
  {
    "objectID": "legacy/three-notebooks-to-jump-start-a-data-science-project.html#introduction",
    "href": "legacy/three-notebooks-to-jump-start-a-data-science-project.html#introduction",
    "title": "Three notebooks to jump start a data science project",
    "section": "",
    "text": "I was in Paris last week for the 2017 Subsurface Hackathon. It was superbly organised by Agile Scientific and Total. Together, they managed to draw a large (about 60 persons) and diverse crowd to participate to the challenge of producing some working and usable software in only two days. The event took place over the weekend just before the EAGE conference, so you could tell people were highly motivated to come and share the experience.\nI was with Martin Bentley in a small team called Water Underground. While Martin was actually sitting in Port Elizabeth in South Africa, I was in La Defense in Total’s brand new offices. Our idea was to mine some groundwater data from the Geological Survey of the Netherlands and use the measurements to predict the evolution of water depth and composition both in space and time. Unfortunately, we did not manage to complete the project. As any data scientist would tell you, the first (and somewhat less rewarding) steps of any data science project can take a lot of time, generally more than expected…"
  },
  {
    "objectID": "legacy/three-notebooks-to-jump-start-a-data-science-project.html#the-data-science-process",
    "href": "legacy/three-notebooks-to-jump-start-a-data-science-project.html#the-data-science-process",
    "title": "Three notebooks to jump start a data science project",
    "section": "The Data Science Process",
    "text": "The Data Science Process\nThere are loads of resources out there that would detail the various stages of a data science project. At the end of the day, any research project would probably follow this sort of process:\n\ndata acquisition\ndata preparation, cleaning and transformation\ndata exploration\nmodelling and predictive analysis\ninterpretation of the results\ncommunication of the results\n\nHaving spent a couple of days on the first three steps of this process, I decided I could share the code and the notebooks I wrote because some of this material is rather generic and can easily be re-used for other projects. There are a few tips and tricks about how to use pandas and Python notebooks for data analysis, so hopefully those can be useful to anyone interested in starting this sort of project.\nThe notebooks are available in a GitHub repository."
  },
  {
    "objectID": "legacy/three-notebooks-to-jump-start-a-data-science-project.html#groundwater-data-in-the-netherlands",
    "href": "legacy/three-notebooks-to-jump-start-a-data-science-project.html#groundwater-data-in-the-netherlands",
    "title": "Three notebooks to jump start a data science project",
    "section": "Groundwater data in the Netherlands",
    "text": "Groundwater data in the Netherlands\nThe dataset we used (and barely started to explore…) is a set of groundwater wells in the Netherlands. Groundwater levels have been measured in some locations on a monthly basis since the 1950’s. So it’s a truly 4D dataset: you can either look at single wells in isolation for time-series analysis, or construct a surface of the water table at a given time by interpolating between points.\nTo come back to the initial steps of the data science project, here are brief descriptions of what can be found in the notebooks for each step.\n\nData acquisition and cleaning\n\nThe data files were selected manually and downloaded from the DINOloket website. The files are provided either as CSV files, or as space-delimited text files. Large headers of variable dimensions make the import into pandas unnecessarily difficult. Additionally, the headers are in Dutch. Thankfully, Martin managed to translate the text and found the logic in the headers. This is all described in the METADATA file on his own GitHub project.\nSome basic exploratory analysis is carried out at the end of the notebook.\nThe notebook for this step is here.\n\n\nData transformation\nThat’s where it starts to be interesting. In this second notebook, I demonstrate how to create a surface from well data by using the Scipy function griddata. Gridding all the points turns out to be meaningless because the measurements do not cover the same period for all the wells.\nResampling the data in time becomes therefore essential. This is where the strength of pandas is obvious as this step can be done with only one line of code.\nOnce the depth measurements have been resampled to follow a constant frequency at all the wells, it becomes very easy to make a selection for a given month and grid up the result. Here is an example:\n\n\n\nSurface of the water table, gridded from well measurements\n\n\n \n\n\nData exploration\nThe third and final notebook provides a convenient way to explore this large dataset. Using the ipywidgets library, I create two interactive plots, one for the surface and one for the time series. A slider can be used to select the date and a dropdown menu gives access to the list of wells.\n\n\n\nSurface of the water table (interactive display in the notebook)"
  },
  {
    "objectID": "legacy/three-notebooks-to-jump-start-a-data-science-project.html#conclusion",
    "href": "legacy/three-notebooks-to-jump-start-a-data-science-project.html#conclusion",
    "title": "Three notebooks to jump start a data science project",
    "section": "Conclusion",
    "text": "Conclusion\nThe hackathon was a great opportunity to meet a lot of people interested in open source software, machine learning and geoscience. I think the time and efforts that went into our project, even it was not completed, should also benefit the wider community. So here is my contribution. If you ever pick up this project where we left it and make some more progress, please get in touch!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "josephbarraud.dev",
    "section": "",
    "text": "Geophysics, Python and More…\n\n\n\n\n\n\nThe following articles were written between 2016 and 2018 for my website geophysicslabs.com. While the website (and the consultancy company I had created) sadly no longer exists, I have moved the old Wordpress site to Github Pages, and converted the files to Quarto documents.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate Globes with Basemap and Cartopy\n\n\n\ngeophysics\n\n\npython\n\n\n\n\n\n\n\nDr Joseph Barraud\n\n\nMar 4, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColormaps and Colorbars\n\n\n\ngeophysics\n\n\npotential-field\n\n\npython\n\n\n\n\n\n\n\nDr Joseph Barraud\n\n\nDec 19, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Load Sandwell & Smith Gravity Data in Python?\n\n\n\ngeophysics\n\n\npotential-field\n\n\npython\n\n\n\n\n\n\n\nDr Joseph Barraud\n\n\nNov 3, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nThree notebooks to jump start a data science project\n\n\n\nblog\n\n\ndata-science\n\n\nmachine-learning\n\n\npython\n\n\n\n\n\n\n\nDr Joseph Barraud\n\n\nJun 19, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to add cross-sections to OpendTect?\n\n\n\ngeophysics\n\n\npython\n\n\nseismic\n\n\n\n\n\n\n\nDr Joseph Barraud\n\n\nFeb 12, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nHow to add maps to OpendTect?\n\n\n\ngeophysics\n\n\npython\n\n\n\n\n\n\n\nDr Joseph Barraud\n\n\nDec 15, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nHillshading with matplotlib\n\n\n\npotential-field\n\n\npython\n\n\n\n\n\n\n\nDr Joseph Barraud\n\n\nJul 13, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nHistogram Equalization in Python and matplotlib\n\n\n\ngeophysics\n\n\npotential-field\n\n\npython\n\n\n\n\n\n\n\nDr Joseph Barraud\n\n\nJul 4, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nBasic statistics with Parkrun data\n\n\n\nblog\n\n\npython\n\n\nstatistics\n\n\n\n\n\n\n\nDr Joseph Barraud\n\n\nJun 18, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nManipulate SEGY files with segy2segy\n\n\n\ngeophysics\n\n\npython\n\n\nseismic\n\n\n\n\n\n\n\nDr Joseph Barraud\n\n\nJun 16, 2016\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "legacy/manipulate-segy-files-with-segy2segy.html",
    "href": "legacy/manipulate-segy-files-with-segy2segy.html",
    "title": "Manipulate SEGY files with segy2segy",
    "section": "",
    "text": "Working with seismic data is great but sometimes technical issues such as loading and converting files come in the way of the more interesting part of the job.\nOne of the most common problems I have encountered in my career is the mismatch between the coordinate system of seismic data and the coordinate system of the rest of my project. In order to visualise all the available data (seismic profiles, gravity and magnetic grids) in a single platform such as OpendTect, they all have to share the same spatial reference. And more often than not, this is not the case. It could be because:\nThere are commercial tools available to perform projections and re-projections of seismic data, and various consulting businesses can provide this service for a fee. Now this can be done for free using the Python tool I have written, segy2segy. This tool is intended to provide in the future more than just projections, but for now, that is what it essentially does."
  },
  {
    "objectID": "legacy/manipulate-segy-files-with-segy2segy.html#installation",
    "href": "legacy/manipulate-segy-files-with-segy2segy.html#installation",
    "title": "Manipulate SEGY files with segy2segy",
    "section": "Installation",
    "text": "Installation\nThe segy2segy script works with Python 2.7 (and should work with Python 3.x) and requires a couple of additional libraries (and of course numpy):\n\nThe ability to read and write SEG-Y files is provided by Obspy.\nThe transformations of coordinates and the projection calculations are handled by GDAL.\n\nI use Anaconda for all my pythonic needs and I found the installation of dependencies straightforward, even for GDAL, which is notoriously difficult to get it to work properly. Please refer to my github page for more detailed instructions about how to install GDAL."
  },
  {
    "objectID": "legacy/manipulate-segy-files-with-segy2segy.html#using-segy2segy",
    "href": "legacy/manipulate-segy-files-with-segy2segy.html#using-segy2segy",
    "title": "Manipulate SEGY files with segy2segy",
    "section": "Using segy2segy",
    "text": "Using segy2segy\nThe tool can be either used on the command line or as a function within a Python script.\nThe tool can process a single file or all the files in a given folder. The input file cannot be overwritten so you either have to provide the name of the output file or a string that will be added as a suffix at the end of the input file name.\nThe syntax of the command line tool is inspired from GDAL programs such as gdalwarp. This is the reason why some of the parameters (-s_srs and -t_srs) are similar.\nA typical example for processing a single file would be:\npython segy2segy.py &lt;\\path\\to\\infile.segy&gt; -o \\path\\to\\output.segy -s_srs 23030 -t_srs 23029\nThe numbers after the -s_srs (source or input coordinate system) and -t_srs (target or output coordinate system) options are EPSG codes. This is the most convenient way to enter references to coordinate systems. Each projection, datum, transformation has a unique standard code that GDAL will recognise and use for the calculation. For example, 23030 is ED50 / UTM zone 30N. There is a useful search engine to get the code you need for your project.\nProcessing all the files in a directory can be done with a single command:\npython segy2segy.py &lt;\\path\\to\\folder&gt; -s_srs 23030 -t_srs 23029 -s_coord CDP -t_coord Source -s _UTM29\nThe -s option tells segy2segy to add “_UTM29” at the end of the input files to create the output. The other parameters of interest are -s_coord and -t_coord. They specify where the coordinates should be found and written in the SEGY files. There are three different locations:\n\nSource coordinate\nGroup coordinate\ncoordinate of ensemble (CDP) position\n\nThe default behaviour of segy2segy is to read the coordinates in the Source location (at byte numbers 73 and 77) and to write the new ones in the CDP location (at byte numbers 181 and 185). The example above does the contrary.\nOnce you have applied the tool successfully, you can check the result in a program such as Seisee, which is very convenient for browsing SEGY headers. Here is an example:\n\n\n\nCoordinate headers in Seisee\n\n\nIn this example, the coordinates in ED50 / UTM30 were read from SRCX and SRCY, converted to ED50 / UTM29 and then written in the CDP-X and CDP-Y columns.\nFinally, the final option is related to the scaler header (SAC column in the Seisee screenshot), which allows you to work for example with centimetre precision even though the coordinates are stored as integers. By default, segy2segy will use and preserve the original scaler present in the input file (byte position 71). You can override that with the -fs (force scaling) and -sc (scaler) options. Keep in mind that if positive, the scaler is used as a multiplier, and when negative, it is used as a divisor.\nI will extend the functionalities of segy2segy in the next few weeks, so stay tuned if you are interested! Please also comment and report issues on the github page.\n \n\n© Crown Copyright. This page contains public sector information licensed under the Open Government Licence v3.0."
  },
  {
    "objectID": "legacy/basic-statistics-with-parkrun-data.html",
    "href": "legacy/basic-statistics-with-parkrun-data.html",
    "title": "Basic statistics with Parkrun data",
    "section": "",
    "text": "This post might not have much to do with geophysics but it is another way to share information and ideas about my interests. I am comparing in  a basic and simple way the results of a 5 km run at several locations in and around Leeds. In the process, I have learnt a couple of things about importing data in Python so I am sharing the code at the end as other fellow runners might find it useful."
  },
  {
    "objectID": "legacy/basic-statistics-with-parkrun-data.html#summary",
    "href": "legacy/basic-statistics-with-parkrun-data.html#summary",
    "title": "Basic statistics with Parkrun data",
    "section": "",
    "text": "This post might not have much to do with geophysics but it is another way to share information and ideas about my interests. I am comparing in  a basic and simple way the results of a 5 km run at several locations in and around Leeds. In the process, I have learnt a couple of things about importing data in Python so I am sharing the code at the end as other fellow runners might find it useful."
  },
  {
    "objectID": "legacy/basic-statistics-with-parkrun-data.html#introduction",
    "href": "legacy/basic-statistics-with-parkrun-data.html#introduction",
    "title": "Basic statistics with Parkrun data",
    "section": "Introduction",
    "text": "Introduction\nLast weekend, I had to change my usual parkrun routine and switch from Roundhay Park to Woodhouse Moor in Leeds. Parkrun is a weekly 5 km run organised every Saturday in hundreds of locations around the UK. It is free and it is timed by volunteers. More information at parkrun.org.uk.\nI am not a very good runner as I have only started to train (more or less…) regularly about a year ago. Parkrun is brilliant for me as it provides that regular rendez-vous with the clock, which is quite useful to check if I have made any progress. Indeed, I am not serious enough with my running to have a proper training plan and to time each of my runs.\nSo, the 5 km parkrun at Roundhay Park, which is the closest to where I live, was cancelled on Saturday to make way for a stage of the World Triathlon Competition. As many of my fellow runners, I had to go to one of the other parkrun locations around Leeds, and I chose Woodhouse Moor as it is the oldest parkrun in Leeds. Saturday was their 459th event!\nTimewise, I did quite well and I established a new Personal Best at 22:54, which is more than 30 seconds better than my previous PB. I suspected I would do better than at Roundhay Park because the course at Roundhay is very challenging with a steep uphill section. Woodhouse Moor has a flat finish and is overall definitely easier on the legs! Or is it really?\nThe time difference between Roundhay Park and Woodhouse Moor got me thinking and I thought I would have a look at some statistics to figure out if there is a genuine difference between the two courses. And since there are lots of data on the parkrun website, I decided to extent my analysis to other parkruns around Leeds and in nearby Bradford. I am therefore comparing in this post the results of the following runs: Roundhay, Woodhouse Moor, Cross Flatts, Temple Newsam, Bramley, and Bradford."
  },
  {
    "objectID": "legacy/basic-statistics-with-parkrun-data.html#descriptive-statistics",
    "href": "legacy/basic-statistics-with-parkrun-data.html#descriptive-statistics",
    "title": "Basic statistics with Parkrun data",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nThe initial idea is to look at some basic exploratory stats such as the mean, min and max values and see if we can draw some obvious conclusions about the difference and similarities between these six parkruns. First, drawing histograms should show us the distribution of running times. All the following plots have the same axes to make the comparison easier.\n     \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParkrun\nRunners\nUnknown %\nMale %\nMean\nStd\nMin\n25%\nMedian\n75%\nMax\n\n\n\n\nWoodhouse #458 04/06/2016\n421\n9.3%\n57.6%\n27:42\n6:33\n15:41\n23:20\n26:56\n30:39\n60:34\n\n\nRoundhay #269 04/06/2016\n356\n5.9%\n58.8%\n28:19\n5:38\n17:45\n24:09\n27:37\n31:08\n47:52\n\n\nTemple Newsam #174 11/06/2016\n223\n5.4%\n66.4%\n28:30\n5:30\n17:32\n24:15\n28:10\n31:53\n43:00\n\n\nCross Flatts #163 11/06/2016\n116\n1.7%\n54.4%\n29:05\n6:56\n18:05\n23:38\n28:30\n33:36\n51:29\n\n\nBramley #52 11/06/2016\n177\n6.8%\n53.3%\n30:27\n7:22\n18:48\n24:50\n29:22\n34:20\n53:04\n\n\nBradford #313 04/06/2016\n391\n5.4%\n56.8%\n30:13\n6:45\n18:04\n25:03\n29:09\n33:52\n55:22\n\n\n\n \nThe table gives a summary of some of the most useful statistics, together with two percentages: the proportion of unknown runners, and the percentage of male runners.\nIt looks like Woodhouse Moor is much “faster” than the others: the best individual time, and the mean and the median values are the smallest of the lot. Besides, the median is systematically smaller than the mean for all the runs, a clear sign that the distributions are asymmetrical. While most runners achieve a time better than about 33 minutes (look at the 75% percentile values), there is a trail of slower runners whose results bring the average up.\nHowever, comparing mean times between different runs might not be statistically significant if the spread of values is too large. Student’s t-test is there to help. The following table shows the p-values of t-tests for the mean time at Woodhouse Moor compared with each of the other parks.\n\n\n\nParkrun\np-value\n\n\n\n\nRoundhay\n0.181\n\n\nTemple Newsam\n0.128\n\n\nCross Flatts\n0.051\n\n\nBramley\n0.000\n\n\nBradford\n0.000\n\n\n\nThe test suggests that the results at Roundhay are actually pretty close to the ones at Woodhouse Moor because there is a relatively large (18.1%) probability of observing a difference as large as the one observed even if the two populations were identical. A p-value less than about 5% would be accepted as more significant.\nSo it is not really clear if Roundhay is more difficult than Woodhouse Moor. Moreover, Roundhay is actually in second position, which is not bad for a hilly course. The type of runners, their age, their degree of fitness could be different in each park. And what about the four other runs, are they “slower” on average because their path is even more difficult? Or is it because their typical runners are also different? To answer these questions, we need to go further into the analysis and take other parameters into consideration."
  },
  {
    "objectID": "legacy/basic-statistics-with-parkrun-data.html#age-grade",
    "href": "legacy/basic-statistics-with-parkrun-data.html#age-grade",
    "title": "Basic statistics with Parkrun data",
    "section": "Age Grade",
    "text": "Age Grade\nTo assess the “quality” of the runners, there is a useful variable, the Age Grade. This is calculated using the time and the age of the runner. By comparing a runner’s performance with the best performer for a given age, the age grade provides a normalised attribute that will allow us to compare runners with each other on more even terms.\n[] [] [][] [][]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParkrun\nRunners\nMale %\nMean\nStd\nMin\n25%\nMedian\n75%\nMax\n\n\n\n\nWoodhouse #458 04/06/2016\n382\n57.6%\n55.8\n9.8\n24.9\n49.1\n55.4\n62.0\n83.1\n\n\nRoundhay #269 04/06/2016\n335\n58.8%\n54.1\n9.3\n28.8\n48.3\n54.0\n60.5\n77.7\n\n\nTemple Newsam #174 11/06/2016\n211\n66.4%\n53.7\n8.9\n33.5\n47.3\n53.7\n60.8\n75.2\n\n\nCross Flatts #163 11/06/2016\n114\n54.4%\n54.7\n10.1\n29.5\n47.4\n53.7\n61.6\n80.1\n\n\nBramley #52 11/06/2016\n165\n53.3%\n50.8\n9.9\n26.5\n43.8\n50.8\n57.7\n73.9\n\n\nBradford #313 04/06/2016\n370\n56.8%\n52.3\n9.9\n27.6\n45.4\n51.6\n59.1\n75.5\n\n\n\nOnce again, the age grades are better on average at Woodhouse Moor (55.8%), compared to anywhere else (54.1% at Roundhay). Let’s look at the p-values:\n\n\n\nParkrun\np-value\n\n\n\n\nRoundhay\n0.019\n\n\nTemple Newsam\n0.011\n\n\nCross Flatts\n0.311\n\n\nBramley\n0.000\n\n\nBradford\n0.000\n\n\n\nBy removing the effect of age and gender in the comparison between the different runs, the age grade has made the statistics much clearer (the p-values are generally smaller). So runners perform better at Woodhouse Moor, whatever their age category or gender. But it could still be due to a different distribution of runners, for example if a lot of well-trained runners prefer to go to Woodhouse Moor rather than Roundhay."
  },
  {
    "objectID": "legacy/basic-statistics-with-parkrun-data.html#effect-of-sampling",
    "href": "legacy/basic-statistics-with-parkrun-data.html#effect-of-sampling",
    "title": "Basic statistics with Parkrun data",
    "section": "Effect of sampling",
    "text": "Effect of sampling\nThe real test to assess the difficulty of a park would be to have exactly the same runners doing each run in a controlled test, ignoring the effect of weather. This is obviously not possible but I tried to simulate the experiment by sampling the results using the same distribution of runners.\nThe runners are distributed in a non-uniform way across the various age categories, as shown on the following bar plot for Roundhay.\n\n\n\nRoundhay Age Categories (#269, 04062016)\n\n\nThe test consists first in sampling randomly 5 runners from each category (actually a selection of them because some categories contain fewer than 5 people). The mean age grade is then calculated and saved. This is done 100 times for each park (Roundhay and Woodhouse Moor). The results are shown with these two histograms of calculated means.\n \nWe have now obtained a clearer distinction between the two parks. The overlap between the two histograms is actually misleading because the mean age grade is better at Woodhouse Moor in 96 samples out of 100 (with p-values smaller than 0.1 in 31% of the cases). So, here you go, my impression that Woodhouse Moor is easier on the legs seems to be supported by the statistics!\nMore could be said about these plots and tables. I will carry on my exploration of the parkrun data in another post. For a start, the same analysis could be repeated for more than one date."
  },
  {
    "objectID": "legacy/basic-statistics-with-parkrun-data.html#importing-data-and-making-plots-with-pandas-and-matplotlib",
    "href": "legacy/basic-statistics-with-parkrun-data.html#importing-data-and-making-plots-with-pandas-and-matplotlib",
    "title": "Basic statistics with Parkrun data",
    "section": "Importing data and making plots with pandas and matplotlib",
    "text": "Importing data and making plots with pandas and matplotlib\nThe analysis and the plots in this post were made in Python, with the help of the pandas and matplotlib libraries. I have written a few functions to read the data and create the plots, they are available in a module simply called parkrun.\nAs a simple example, here is the way to import a table of weekly results in pandas. First, you need to save the results in a text file. Once you are on your favourite parkrun website looking at the page of results, select and copy everything on the page (ctrl+A and ctrl+C), including the headers and the text at the bottom. Paste the results in a blank text file and save. The function importResults will convert this in a pandas DataFrame.\nimport parkrun\ninfile = r'.\\\\data\\\\Parkrun\\_Woodhouse\\_Results\\_458\\_04062016.txt' \nresults = parkrun.importResults(infile,report=True)\nNote: For this to work properly, make sure you copy the page when it is displaying the banner at the top, otherwise the number of lines to skip at the beginning of the file will not match what is hard-coded in the function. But you can in fact modify this behaviour with the skiprows option.\nNext, in order to plot the histogram of times, use the time_hist function as follows:\nparkrun.time_hist(results,title='Woodhouse Moor',style='ggplot')\nThis creates the following plot (the previous plots were made with the ‘bmh’ style).\n\nPlease see the github repository for more information about the other functions."
  },
  {
    "objectID": "legacy/colormaps-and-colorbars.html",
    "href": "legacy/colormaps-and-colorbars.html",
    "title": "Colormaps and Colorbars",
    "section": "",
    "text": "Every now and then, the geophysics community debates furiously about which colormap is best to display geophysical data. This is not a new issue and the starting point for this discussion seems to be roughly 1996 when Rogowitz and Treinish started to question the use of rainbow-like colormaps for scientific visualisation.\nDespite its obvious flaws, the rainbow (or “jet”) colormap is still used a lot, especially in geosciences. Rainbow-slayers regularly try to kill the beast, but it keeps coming back! A major blow in the rainbow came two years ago with the adoption by both MATLAB and matplotlib of new default colormaps. The latest attempt comes from Matt Hall in a well-argued post."
  },
  {
    "objectID": "legacy/colormaps-and-colorbars.html#the-reasons-for-the-love",
    "href": "legacy/colormaps-and-colorbars.html#the-reasons-for-the-love",
    "title": "Colormaps and Colorbars",
    "section": "The reasons for the love",
    "text": "The reasons for the love\nWhy do some geophysicists still like rainbow-like colormaps? I am not going to list all the reasons why old habits are still persistent (it’s probably because they are old habits…), but I would like to make the case for one particular use that is more relevant to my experience: potential fields data. The interpretation of this sort of data is all about describing anomalies: they can be negative and positive, big and small; and then there are extreme anomalies… Ideally we would like to visualise all these different types of anomalies at the same time, in a single map. That can become quickly impossible with perceptually uniform colormaps.\nHere is an example with some magnetic data from an airborne survey that was completed for the USGS over an area in Colorado (Bankey and Grauch, 2004). The acquisition report and the data can be downloaded from this page. In the following plots, I am using the gridded Reduced-To-Pole magnetic anomalies derived by the USGS from the measurements. Also, all the maps in this post have been produced with interpies, a Python package that I have created to help geophysicist (and me!) to process and display their data.\nBefore making a map of the grid, it’s a good idea to look at the distribution of anomalies (Figure 1). The distribution is quite uneven, with the bulk of the values situated between -600 and -200 nT. The histogram is skewed towards the right and there is a long tail of positive anomalies up to +300 nT.\n\n\n\n\n\n\nFigure 1: Normed histogram and KDE of magnetic anomalies. The x-axis is in nT.\n\n\n\nNow, following advocates of perceptually uniform colormaps, here is a map of the anomalies using viridis, the default colormap in matplotlib (Figure 2).\n\n\n\n\n\n\nFigure 2: Magnetic anomalies from the Blanca survey. Viridis colormap, no hillshade.\n\n\n\nSo yes, the linear increase in brightness of viridis provides a coherent representation of the distribution we saw earlier. Most of the anomalies are in the blue-greenish zone around the mean at -418. Bright yellow spots indicate the highs and how localised they are.\nWe create maps for a purpose. So if the purpose is to locate only the highest anomalies, then this map is suitable. However, most interpreters would like to go further."
  },
  {
    "objectID": "legacy/colormaps-and-colorbars.html#diverging-colormaps-for-anomalies",
    "href": "legacy/colormaps-and-colorbars.html#diverging-colormaps-for-anomalies",
    "title": "Colormaps and Colorbars",
    "section": "Diverging colormaps for anomalies",
    "text": "Diverging colormaps for anomalies\nAnomalies on either sides of a central “mean” are ideally mapped with diverging colormaps. And rainbow is essentially a diverging colormap: the central green-yellow is the brightest colour, and the blue and red ends are mapping minimum and maximum values, i.e. negative and positive anomalies. The standard “geosoft” colormap is widely used in the grav-mag community. Let’s see how it performs with these data (Figure 3).\n\n\n\n\n\n\nFigure 3: Magnetic anomalies displayed with the geosoft colormap.\n\n\n\nThe result is not great, certainly not superior to the previous example. So maybe changing the colormap was not the best option to obtain a good visualisation."
  },
  {
    "objectID": "legacy/colormaps-and-colorbars.html#dealing-with-uneven-distributions",
    "href": "legacy/colormaps-and-colorbars.html#dealing-with-uneven-distributions",
    "title": "Colormaps and Colorbars",
    "section": "Dealing with uneven distributions",
    "text": "Dealing with uneven distributions\nMagnetic properties of rocks can vary greatly so it is not surprising to see such extreme distributions. The common approach is therefore to “re-balance” the distribution to minimise the importance of the extremes. Graphically, this can be done by histogram equalisation.\nTraditionally, this normalisation would be applied to the data. In interpies, I have experimented a different approach: instead of modifying the data, the colormap is modified to visually achieve the same result (Figure 4).\n\n\n\n\n\n\nFigure 4: Magnetic anomalies displayed using histogram equalisation and the viridis colormap.\n\n\n\nThe result is that more details are now visible in the areas away from the big positive anomalies. The important part in this map is the colorbar: it clearly indicates both the range of values (as before) but also the data distribution. A large range of anomalies are shown with the same yellow but that is the price to pay to see the other anomalies. Moreover, this manipulation is not hidden, contrary to the case where the data range is simply clipped."
  },
  {
    "objectID": "legacy/colormaps-and-colorbars.html#hillshading",
    "href": "legacy/colormaps-and-colorbars.html#hillshading",
    "title": "Colormaps and Colorbars",
    "section": "Hillshading",
    "text": "Hillshading\nMore options are available to create good maps. Inevitably, with or without equalisation, portions of the map will show uniform colours, despite the presence of small variations. Adding hillshade is generally the best way to highlight details and structures in those areas (Figure 5).\n\n\n\n\n\n\nFigure 5: Magnetic anomalies displayed with histogram equalisation, the geosoft colormap and hillshading. Statistics are shown on the colorbar (mean and 2*sigma).\n\n\n\nI think this map is a good example where the geosoft colormap shows its interest: it has a bright pink top end that, when associated with histogram equalisation, takes on all the extreme positive anomalies, leaving the “normal” rainbow from blue to red to represent the bulk of the data. The colormap and the equalisation therefore work together to create the impression of uniform data distribution."
  },
  {
    "objectID": "legacy/colormaps-and-colorbars.html#use-the-colorbar",
    "href": "legacy/colormaps-and-colorbars.html#use-the-colorbar",
    "title": "Colormaps and Colorbars",
    "section": "Use the colorbar",
    "text": "Use the colorbar\nNobody should be fooled by the visualisation tricks used to make the previous map. The colorbar is there to indicate the real distribution of the anomalies. I have also added an option in interpies that allows you to put the mean and the standard deviation as a scale that should be more meaningful than linearly spaced ticks (Figure 5)."
  },
  {
    "objectID": "legacy/colormaps-and-colorbars.html#conclusion",
    "href": "legacy/colormaps-and-colorbars.html#conclusion",
    "title": "Colormaps and Colorbars",
    "section": "Conclusion",
    "text": "Conclusion\nManipulating the data or the colormap should not be a problem if the method and the intention are clearly described with the map. Why not making several maps? Reports are electronic, articles can have appendices. That does not cost anything. A first map with a perceptual colormap and no normalisation can give a sense of scale. A second map like the one in figure 5 can be used for qualitative interpretation and to differentiate anomalies.\nA colormap should be selected with care, but it depends on the data. Some colormaps distort the perception in order to highlight certain features. But filters and transformations have the same purpose: their application is used to extract information from the data and to eventually tell a story. Careful though: applying both non-perceptual colormaps and filters should be avoided because this might create some confusion if they do not work together."
  },
  {
    "objectID": "legacy/colormaps-and-colorbars.html#more-examples",
    "href": "legacy/colormaps-and-colorbars.html#more-examples",
    "title": "Colormaps and Colorbars",
    "section": "More examples",
    "text": "More examples\nI have created a Jupyter notebook that presents most of the options available in interpies to make maps. The module is based on matplotlib and allows you to create complex displays with just one function and its parameters."
  },
  {
    "objectID": "legacy/colormaps-and-colorbars.html#reference",
    "href": "legacy/colormaps-and-colorbars.html#reference",
    "title": "Colormaps and Colorbars",
    "section": "Reference",
    "text": "Reference\nBankey, V., Grauch, V.J.S., 2004. Digital aeromagnetic data and derivative products from a helicopter survey over the town of Blanca and surrounding areas, Alamosa and Costilla counties, Colorado, Open-File Report.\nKovesi, P., 2015. Good Colour Maps: How to Design Them. arXiv:1509.03700 [cs.GR] 2015"
  },
  {
    "objectID": "legacy/how-to-add-maps-to-opendtect.html",
    "href": "legacy/how-to-add-maps-to-opendtect.html",
    "title": "How to add maps to OpendTect?",
    "section": "",
    "text": "Note\n\n\n\nThis post is associated with a couple of Jupyter notebook available on GitHub.\n\n\nOpendTect is a great piece of software that allows you to load, process and interpret seismic data. In OpendTect, 2D lines and 3D volumes are displayed in a nice 3D environment that is easy to manipulate. Horizons, either 2D (lines) or 3D (surfaces) can be added to the 3D view alongside the seismic data.\nIn fact, any grid, not just seismic horizons, can be loaded in the OpendTect environment. This makes it ideal for displaying data from gravity and magnetic surveys. As a potential-field specialist, I am then able to compare directly the position of gravity and magnetic anomalies with the features that I see on seismic data.\nWhile loading grids in OpendTect could be a tutorial on its own, in this post I will actually add one more difficulty by explaining how to load pictures, typically geological maps, to the 3D view. The solution I am proposing allows you to retain the original colours of the map (or at least most of them).\nA map is typically created in a program like ArcGIS or QGIS and exported as an RGB coloured image, therefore containing 3 channels or bands (red, green and blue). The problem is that OpendTect cannot handle this type of image, it deals only with basic one-band grids. When they are rendered by OpendTect, one-band grids may appear in colours because the data are colour-coded (or mapped) to a list of colours, a colormap. Colormaps therefore help us to visualise the various intensities of the quantity contained in the grid. Note that we could also display a one-band grid with a grayscale, demonstrating it carries only intensity information.\nSo the trick to display RGB images in OpendTect is to merge the three bands into one, trying in the process not to lose too much information about the colours. This process is called colour quantization, or colour-depth reduction, and consists in finding for each colour in an image its closest match in a limited set of (predefined or not) colours.\nThe result is also called an indexed colour image because the colour information is stored in a separate file called a palette. The one-band grid therefore contains indices (or positions) representing a colour in the palette.\nThere are many methods and algorithms to achieve this result and the one I am proposing is specially targeting geological maps. These maps already contain a relatively small number of different colours: they represent the various types of rocks in an area, and, unless your area is particularly complex, there should be fewer than a hundred types (or ages, or whatever has been mapped). So the quantization should work pretty well in this case."
  },
  {
    "objectID": "legacy/how-to-add-maps-to-opendtect.html#introduction",
    "href": "legacy/how-to-add-maps-to-opendtect.html#introduction",
    "title": "How to add maps to OpendTect?",
    "section": "",
    "text": "Note\n\n\n\nThis post is associated with a couple of Jupyter notebook available on GitHub.\n\n\nOpendTect is a great piece of software that allows you to load, process and interpret seismic data. In OpendTect, 2D lines and 3D volumes are displayed in a nice 3D environment that is easy to manipulate. Horizons, either 2D (lines) or 3D (surfaces) can be added to the 3D view alongside the seismic data.\nIn fact, any grid, not just seismic horizons, can be loaded in the OpendTect environment. This makes it ideal for displaying data from gravity and magnetic surveys. As a potential-field specialist, I am then able to compare directly the position of gravity and magnetic anomalies with the features that I see on seismic data.\nWhile loading grids in OpendTect could be a tutorial on its own, in this post I will actually add one more difficulty by explaining how to load pictures, typically geological maps, to the 3D view. The solution I am proposing allows you to retain the original colours of the map (or at least most of them).\nA map is typically created in a program like ArcGIS or QGIS and exported as an RGB coloured image, therefore containing 3 channels or bands (red, green and blue). The problem is that OpendTect cannot handle this type of image, it deals only with basic one-band grids. When they are rendered by OpendTect, one-band grids may appear in colours because the data are colour-coded (or mapped) to a list of colours, a colormap. Colormaps therefore help us to visualise the various intensities of the quantity contained in the grid. Note that we could also display a one-band grid with a grayscale, demonstrating it carries only intensity information.\nSo the trick to display RGB images in OpendTect is to merge the three bands into one, trying in the process not to lose too much information about the colours. This process is called colour quantization, or colour-depth reduction, and consists in finding for each colour in an image its closest match in a limited set of (predefined or not) colours.\nThe result is also called an indexed colour image because the colour information is stored in a separate file called a palette. The one-band grid therefore contains indices (or positions) representing a colour in the palette.\nThere are many methods and algorithms to achieve this result and the one I am proposing is specially targeting geological maps. These maps already contain a relatively small number of different colours: they represent the various types of rocks in an area, and, unless your area is particularly complex, there should be fewer than a hundred types (or ages, or whatever has been mapped). So the quantization should work pretty well in this case."
  },
  {
    "objectID": "legacy/how-to-add-maps-to-opendtect.html#method",
    "href": "legacy/how-to-add-maps-to-opendtect.html#method",
    "title": "How to add maps to OpendTect?",
    "section": "Method",
    "text": "Method\nTo summarise what needs to be done to get a coloured map into OpendTect, here is a breakdown of the method:\n\nPrepare the RGB image of the map.\nConvert the image from RGB to indexed colour using color quantization.\nCrop and resample the image to fit the OpendTect survey area.\nImport into OpendTect.\n\nThe first step could involve extracting a picture from a PDF file, or exporting an image from a GIS application. Additionally, the map might also need to be georeferenced and/or projected in the same coordinate system as the seismic data. We will assume in the following that this stage has already been completed."
  },
  {
    "objectID": "legacy/how-to-add-maps-to-opendtect.html#example-the-kevitsa-deposit-northern-finland",
    "href": "legacy/how-to-add-maps-to-opendtect.html#example-the-kevitsa-deposit-northern-finland",
    "title": "How to add maps to OpendTect?",
    "section": "Example: the Kevitsa Deposit, northern Finland",
    "text": "Example: the Kevitsa Deposit, northern Finland\nIn order to describe the method more efficiently, I am using an example based on some data that have recently been made freely available thanks to the Frank Arnott award. It is a complete geophysical dataset that has been used for the exploration of the Kevitsa intrusion in Finland (Malehmir et al., 2012). It contains a 3D reflection seismic survey, potential-field survey data, wells, geological maps and cross-sections.\nThe mafic-ultramafic intrusion has an elliptical shape in map view and the 3D seismic survey is located on its north-east side (Figure 1). The intrusion is surrounded by sedimentary and volcanic rocks that are both folded and faulted. The seismic data were used to image the contact between the intrusion and the adjacent units, as well as the geometry of the intrusion at depth (Koivisto et al., 2015).\n\n\n\n\n\n\nFigure 1: Geological map of the Kevitsa area, with the 3D reflection seismic survey area shown in red (after Koivisto et al., 2015).\n\n\n\nSo the purpose of this exercise is to have an image of the geological map rendered together with the seismic data in a 3D environment. The first stage of this process is to extract a portion of this map centred on the 3D survey. The legend, the scale and other annotations are superfluous for this purpose, as only the geological information is required (Figure 2).\n\n\n\n\n\n\nFigure 2: Image of the geological map centred on the 3D survey.\n\n\n\n\nGeoreferencing\nIt is essential for the final step of the method (interpolation onto the OpendTect survey grid) to have the geographic coordinates of the image, i.e. its location and its extent. This information will be contained in a world file that is automatically created by QGIS or by ArcGIS when the map is exported to a PNG file. Look for a small .pgw text file with the same name as the PNG file. These two files (the .png and the .pgw) always need to sit together on your drive."
  },
  {
    "objectID": "legacy/how-to-add-maps-to-opendtect.html#color-quantization",
    "href": "legacy/how-to-add-maps-to-opendtect.html#color-quantization",
    "title": "How to add maps to OpendTect?",
    "section": "Color Quantization",
    "text": "Color Quantization\nThis is the conversion of our RGB image to a one-channel image using a specific set of colours.\n\nPalette\n\n\n\n\n\n\nFigure 3: The 256 colours of the Windows 8-bit colour palette.\n\n\n\nI am using a fixed palette of 256 colours. Fixing the colours can potentially degrade the performance of the quantization process, but this is essential for OpendTect to render our images consistently. It is also much simpler, as the alternative would be to have a different palette for each image, which is impractical.\nThis palette is the classic Windows palette (Figure 3). It contains a number of shades of red, green and blue-ish colours, and also the typical set of basic colours that are found in lots of Windows programs. A text file with the RGB colours of the palette can be found here.\n\n\nPairwise distance\nThe quantization can be simply performed with a function of the scikit-learn Python library. Its metrics sub-module contains functions to compute distances between objects. We need a function that can tell us the colour in the palette that is the closest to the colour of each pixel in our RGB image.\nThis function is called pairwise_distances_argmin and using it for color quantization is straightforward. Here is the gist of the method in Python code:\nfrom sklearn.metrics import pairwise_distances_argmin\n\nindices = pairwise_distances_argmin(flat_array, win256)\nindexedImage = indices.reshape((nrows, ncols))\nHere, win256 is a 3-column array containing the 256 colours of the Windows palette. It is compared with flat_array, which is a reshaped version of our initial RGB image. The result is a list of indices pointing at the colours in our palette. To get our final grid, we finally need to reshape the result back to a rectangular array that has the same dimensions as the initial image.\nA more complete version of the code is available in this Jupyter notebook.\nThe result is our quantization method is quite good and looks almost identical to the original (Figure 4). Of course, in order to display it properly we also need to have the Windows 8-bit palette loaded as a colormap in matplotlib (see the notebook for the details).\n\n\n\n\n\n\nFigure 4: Indexed-colour image of the Kevitsa geological map."
  },
  {
    "objectID": "legacy/how-to-add-maps-to-opendtect.html#cropping-and-resampling",
    "href": "legacy/how-to-add-maps-to-opendtect.html#cropping-and-resampling",
    "title": "How to add maps to OpendTect?",
    "section": "Cropping and Resampling",
    "text": "Cropping and Resampling\nHaving the RGB image converted to a one-band grid is only one part of the process of importing the map into OpendTect. The other non-trivial bit is to make sure the position and sampling of the grid correspond exactly with the grid that defines the “Survey” in OpendTect. As the OpendTect documentation puts it: “Projects are organized in Surveys - geographic areas with a defined grid that links X,Y co-ordinates to inline, crossline positions. 3D seismic volumes must lie within the defined survey boundaries.”\nFrom this point onwards, there are two possibilities: either you have an actual 3D seismic volume loaded in your project, or you have only 2D seismic lines. In the first scenario, the 3D volume dictates the geometry of the OpendTect survey. In the latter case, you are actually free to define the survey grid, since “2D lines and wells are allowed to stick outside the survey box”.\n\nSurvey definition\nSo for the simple 2D case, my advice is generally to use a grid that defines a rectangle area that encompasses most or all the 2D lines. The grid needs to be in the same projected coordinate system as the seismic. The common scenario is that a gravity or a magnetic survey is available in the area, so this is the grid that should be used for the OpendTect survey.\n\n\n\n\n\n\nFigure 5: Screenshot from OpendTect showing the parameters used to define the survey area that corresponds to the Kevitsa 3D seismic data.\n\n\n\nIn contrast, 3D seismic surveys come in all sorts of shape and orientation so a mismatch is likely to occur between the survey grid and the map we want to import. Cropping and resampling the map are therefore necessary.\nIn the Kevitsa example, the outline of the seismic cube makes an angle of about 20 degrees with the north. In OpendTect, the Survey grid is created by scanning the coordinates of the traces in the SEGY file. The result shows that the grid comprises 280 in-lines and 287 cross-lines (Figure 5).\n\n\nInterpolation\nCropping and resampling are performed in one pass by interpolation onto the grid that defines the location of the OpendTect survey. The SciPy interpolate module contains the functions we need for this purpose.\nThe important task before running the interpolation is to create two sets of coordinates: one for the grid of our image and one for the target grid, i.e. the seismic survey.\nAs stated earlier, the coordinates of our input image were exported by QGIS when we created the image of the map. In Python, we load the image and its attached geographic metadata with the rasterio module (see Jupyter notebook). We could also directly open the .pgw file in a text editor and read the cell size and location of the upper left pixel.\nKeep in mind when reading coordinates of raster images that these numbers could correspond to either the centre or the corner of the pixel. There are two conventions for registering images (gridline- and pixel-based) and both are equally used. For example, rasterio would give you the position of the corner of the upper-left pixel, while the world file gives you its centre.\nIn any case, constructing a grid of coordinates with Numpy is easy thanks to the meshgrid function:\n# 1-D arrays of coordinates\nx = np.linspace(xmin,xmax,num=ncols,endpoint=False)\ny = np.linspace(ymin,ymax,num=nrows,endpoint=False)\n# 2-D arrays of coordinates\nX,Y = np.meshgrid(x,y)\nY = np.flipud(Y)\nThe X and Y arrays give the coordinates (X[i,j], Y[i,j]) of each pixel (i,j) in the image. Flipping the Y array upside-down is necessary because indices are counted from the top down, while the y-coordinate (northing) increases northward.\nThe arrays for the target grid are created slightly differently. First we create arrays of in-line and cross-line indices (trace numbers) as defined in OpendTect. Then the coordinates are calculated using the formulas of the affine transformation that are provided in Survey Setup &gt; Coordinate settings (advanced panel, Figure 6).\n\n\n\n\n\n\nFigure 6: Equations of the affine transformation that gives X and Y coordinates from inline and crossline numbers.\n\n\n\nThe coordinates of the target grid are therefore given by:\ninline_limits = np.arange(1000,1280,1)\nxline_limits = np.arange(1000,1287,1)\ninline,xline = np.meshgrid(inline_limits,xline_limits,indexing='ij')\n# indexing starts from bottom-left corner\ninline = np.flipud(inline)\n# Now we can compute the coordinates\nXi = 3491336.248 - 3.19541219*inline + 9.4758042*xline\nYi = 7497848.4 + 9.47383513*inline + 3.19552448*xline\nThe last stage consists in combining all these elements together by first creating the interpolation function with the first grid and then running it onto the second grid.\npoints = np.column_stack((X.flatten(),Y.flatten()))\nvalues = indexedImage.flatten()\ninterp = interpolate.NearestNDInterpolator(points,values)\nnewImage = interp((Xi,Yi))\nIt is important to use the nearest-neighbour interpolator here because we need to preserve the values of our indexed colours. Otherwise, the colours of our image could change in an unexpected way!\nThe result looks great and shows the map rotated in the frame of the 3D seismic survey (Figure 7). Note that the image is a bit bigger than the outline of the 3D because I have extended the target grid by 50 pixels on all sides (see the Jupyter notebook for the code to achieve that).\n\n\n\n\n\n\nFigure 7: Geological map of the Kevitsa area rotated in the local coordinate system of the seismic survey."
  },
  {
    "objectID": "legacy/how-to-add-maps-to-opendtect.html#importing-the-grid-into-opendtect",
    "href": "legacy/how-to-add-maps-to-opendtect.html#importing-the-grid-into-opendtect",
    "title": "How to add maps to OpendTect?",
    "section": "Importing the grid into OpendTect",
    "text": "Importing the grid into OpendTect\nThe final step is to actually import the grid of our indexed-colour rotated image into OpendTect. The grid will be imported as a 3D horizon geometry and the colour information will be imported as an attribute. Both operations can be performed at the same time. But first, our grid needs to be saved in a format that OpendTect can read, the simplest one being an ASCII column format.\nSince we have deliberately created the new image to match the grid of the survey data, the inline and crossline trace numbers can be used instead of the X and Y coordinates. The code to make the ASCII text file is available in the Jupyter notebook.\nImporting the file in OpendTect is easy:\n\nGo to Survey &gt; Import  &gt; Horizon &gt; ASCII &gt; Geometry 3D…\nSelect the newly created .xyz file.\nAdd the colour indices (the pixel values) as an attribute called “Geology”.\nDefine the format by clicking “Define…”.\nSelect “Inl Crl” instead of “X Y” in the dropdown menu.\n\nThe rest of the format definition should automatically be correct since we have added a Z column in the third position. Define the name of the output horizon and the Import Horizon window should look like Figure 8.\n\n\n\n\n\n\nFigure 8: Import Horizon window in OpendTect"
  },
  {
    "objectID": "legacy/how-to-add-maps-to-opendtect.html#displaying-the-map-in-opendtect",
    "href": "legacy/how-to-add-maps-to-opendtect.html#displaying-the-map-in-opendtect",
    "title": "How to add maps to OpendTect?",
    "section": "Displaying the map in OpendTect",
    "text": "Displaying the map in OpendTect\nOur new 3D horizon can now be added to the display by clicking on “3D Horizon” in the scene Tree, then click “Add…”. Select the horizon in the list and click OK. You should see a flat plane at depth = 0 with a single bright colour. This is because OpendTect displays the Z-values of the horizon by default. To show the actual colours of the geological map, right-click on “Z value” in the Tree, then “Select Attribute” and “Horizon Data (1)…”. Select “Geology” in the list.\n\nLoading the correct colormap\nThe map should now be displayed in the 3D scene in the right location related to the seismic data. However, the colours are likely to be completely wrong! This is because there is one last piece missing: the Windows 8-bit palette we used for the quantization. It needs to be imported as a new “ColorTable” (Figure 9). The file is available here.\n\n\n\n\n\n\nFigure 9: Loading the Windows 8-bit palette into OpendTect\n\n\n\nTo import the correct palette, follow these steps:\n\ngo to Survey &gt; Import &gt; ColorTable…\nSelect Import from: File.\nBrowse to the location of the text file that contains the color table\nSelect “win_256” in the list of “Color table(s) to add”\n\nThe final task is to assign the new Color Table to the “Geology” attribute. Select the attribute in the scene tree, then choose “Win_256” in the dropdown list of ColorTables. Make sure the range  of the color scale goes from 0 to 255.\nEt voila! The map is not displayed in all its glory together with the seismic data of the 3D survey (Figure 10 and Figure 11).\n\n\n\n\n\n\nFigure 10: 3D view of the geological map in OpendTect.\n\n\n\n\n\n\n\n\n\nFigure 11: 3D view of the geological map with seismic data of the Kevitsa 3D survey."
  },
  {
    "objectID": "legacy/how-to-add-maps-to-opendtect.html#conclusion",
    "href": "legacy/how-to-add-maps-to-opendtect.html#conclusion",
    "title": "How to add maps to OpendTect?",
    "section": "Conclusion",
    "text": "Conclusion\nWhile not completely straightforward, importing geological maps in OpendTect is possible! ;-)"
  },
  {
    "objectID": "legacy/how-to-add-maps-to-opendtect.html#acknowledgements",
    "href": "legacy/how-to-add-maps-to-opendtect.html#acknowledgements",
    "title": "How to add maps to OpendTect?",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe geological map and the 3D seismic survey were kindly made available through the Frank Arnott Award by First Quantum Minerals Ltd. The content of the dataset is re-used here with permission."
  },
  {
    "objectID": "legacy/how-to-add-maps-to-opendtect.html#references",
    "href": "legacy/how-to-add-maps-to-opendtect.html#references",
    "title": "How to add maps to OpendTect?",
    "section": "References",
    "text": "References\nKoivisto, E., Malehmir, A., Hellqvist, N., Voipio, T., Wijns, C., 2015. Building a 3D model of lithological contacts and near-mine structures in the Kevitsa mining and exploration site, Northern Finland: Constraints from 2D and 3D reflection seismic data. Geophysical Prospecting 63, 754–773. doi:10.1111/1365-2478.12252\nMalehmir, A., Juhlin, C., Wijns, C., Urosevic, M., Valasti, P., Koivisto, E., 2012. 3D reflection seismic imaging for open-pit mine planning and deep exploration in the Kevitsa Ni-Cu-PGE deposit, northern Finland. Geophysics 77, WC95-WC108. doi:10.1190/geo2011-0468.1"
  },
  {
    "objectID": "legacy/how-to-load-ss-gravity-data-in-python.html",
    "href": "legacy/how-to-load-ss-gravity-data-in-python.html",
    "title": "How to Load Sandwell & Smith Gravity Data in Python?",
    "section": "",
    "text": "Mapping the depth of the oceans globally was one of the greatest successes of geophysics in the 20th century. Without bathymetry, we would not know the locations of mid-ocean ridges, volcanic seamounts, and transform faults, to name a few, all of which are key elements of plate tectonics.\nBut mapping the seafloor is hard, tedious and costly - shipborne measurements, while accurate, cover only tiny portions of the ocean surface. And the distribution of those measurements (made first by manual sounding, later with sonars) is very uneven: large parts of the oceans, especially in the southern hemisphere remain completely unexplored. In fact, only 8% of the oceanic surface is mapped accurately (Smith et al., 2017).\nThe best solution available to us to fill the gaps is satellite altimetry (Smith and Sandwell, 1997). The bathymetry is derived from gravity anomalies that are calculated from measurements of the sea surface height. The basic principle behind this technique is that the underwater topography (ridges and troughs) induce small changes in the gravity pull around them, deforming the sea surface.\nThe height of the sea surface can be measured very accurately by specialised satellites, which have seen their number and quality increase significantly in the last 10 years. The latest versions of global gravity anomaly maps (Sandwell and Smith version 24.1, DTU version 15, Getech’s Multi-Sat) show remarkable improvements over the previous ones (Sandwell et al., 2014; Andersen and Knudsen, 2016).\nIn this post, I explain how to obtain global gravity data and how to display them in a map with Python."
  },
  {
    "objectID": "legacy/how-to-load-ss-gravity-data-in-python.html#introduction",
    "href": "legacy/how-to-load-ss-gravity-data-in-python.html#introduction",
    "title": "How to Load Sandwell & Smith Gravity Data in Python?",
    "section": "",
    "text": "Mapping the depth of the oceans globally was one of the greatest successes of geophysics in the 20th century. Without bathymetry, we would not know the locations of mid-ocean ridges, volcanic seamounts, and transform faults, to name a few, all of which are key elements of plate tectonics.\nBut mapping the seafloor is hard, tedious and costly - shipborne measurements, while accurate, cover only tiny portions of the ocean surface. And the distribution of those measurements (made first by manual sounding, later with sonars) is very uneven: large parts of the oceans, especially in the southern hemisphere remain completely unexplored. In fact, only 8% of the oceanic surface is mapped accurately (Smith et al., 2017).\nThe best solution available to us to fill the gaps is satellite altimetry (Smith and Sandwell, 1997). The bathymetry is derived from gravity anomalies that are calculated from measurements of the sea surface height. The basic principle behind this technique is that the underwater topography (ridges and troughs) induce small changes in the gravity pull around them, deforming the sea surface.\nThe height of the sea surface can be measured very accurately by specialised satellites, which have seen their number and quality increase significantly in the last 10 years. The latest versions of global gravity anomaly maps (Sandwell and Smith version 24.1, DTU version 15, Getech’s Multi-Sat) show remarkable improvements over the previous ones (Sandwell et al., 2014; Andersen and Knudsen, 2016).\nIn this post, I explain how to obtain global gravity data and how to display them in a map with Python."
  },
  {
    "objectID": "legacy/how-to-load-ss-gravity-data-in-python.html#get-the-data",
    "href": "legacy/how-to-load-ss-gravity-data-in-python.html#get-the-data",
    "title": "How to Load Sandwell & Smith Gravity Data in Python?",
    "section": "Get the data",
    "text": "Get the data\nThe global marine gravity data from the Scripps Institution of Oceanography in San Diego is commonly known as the “Sandwell and Smith” gravity dataset, even though several other authors have contributed to its development since the 1990’s. The latest version, v24.1, can be downloaded on this FTP site. The file that we need is grav.img.24.1.\nThis large file (712 MB) actually contains a map of the gravity anomalies, projected with a spherical Mercator projection."
  },
  {
    "objectID": "legacy/how-to-load-ss-gravity-data-in-python.html#convert-the-data-using-gmt",
    "href": "legacy/how-to-load-ss-gravity-data-in-python.html#convert-the-data-using-gmt",
    "title": "How to Load Sandwell & Smith Gravity Data in Python?",
    "section": "Convert the data using GMT",
    "text": "Convert the data using GMT\nTo obtain the grid of gravity anomalies in a geographic coordinate system, the img file needs to be “unprojected” and converted to a different format. This can be done with a special tool available in GMT (Generic Mapping Tools). The following command will convert the img file to a new file called grav_v24.nc. The -R option is normally used to extract portion of the grid, but here the values cover the entire extent of the grid. The -S returns the data in mGal by multiplying the data by 0.1. Finally, the -V option provides a verbose output, so that we can see what is going on.\nimg2grd grav.img.24.1 -Ggrav_v24.nc -R0/360/-80.738/80.738 -S0.1 -V\nThe output of this command shows the following (I used version 5.4.2):\nimg2grd: Expects grav.img.24.1 to be 21600 by 17280 pixels spanning 0/360.0/-80.738009/80.738009.\nimg2grd: To fit [averaged] input, your grav.img.24.1 is adjusted to -R0/360/-80.738008628/80.738008628.\nimg2grd: The output grid size will be 21600 by 17280 pixels.\nimg2grd: Created 21600 by 17280 Mercatorized grid file. Min, Max values are -366.39999 943.79999\nimg2grd: Undo the implicit spherical Mercator -Jm1i projection.\ngrdproject: Processing input grid\ngrdproject: Transform (0/360/-80.738/80.738) &lt;-- (0/360/0/287.999892786) [inch]\ngrdproject: gmt_grd_project: Output grid extrema [-957.6/967.8] exceed extrema of input grid [-366.4/943.8] due to resampling\ngrdproject: gmt_grd_project: See option -n+c to clip resampled output range to given input range\ngrdproject: Proj4 string to be converted to WKT:\n +proj=longlat +no_defs\nNote that the img2grd command can only output the result in the default format of GMT, which is netCDF."
  },
  {
    "objectID": "legacy/how-to-load-ss-gravity-data-in-python.html#convert-to-geotiff",
    "href": "legacy/how-to-load-ss-gravity-data-in-python.html#convert-to-geotiff",
    "title": "How to Load Sandwell & Smith Gravity Data in Python?",
    "section": "Convert to Geotiff",
    "text": "Convert to Geotiff\nThe next step would typically be to load the grid, for example in a Numpy array if you work in Python. A powerful library for reading raster data in Python is rasterio. Unfortunately, the underlying library of rasterio, GDAL, that provides the drivers to read raster data, fails to load the netCDF that we have just created. So an additional step is necessary in order to convert this file to a Geotiff. I have used this GMT command successfully:\ngrdconvert grav_v24.nc -Ggrav_v24.tif=gd:GTiFF -V"
  },
  {
    "objectID": "legacy/how-to-load-ss-gravity-data-in-python.html#resample-to-a-square-cell-size",
    "href": "legacy/how-to-load-ss-gravity-data-in-python.html#resample-to-a-square-cell-size",
    "title": "How to Load Sandwell & Smith Gravity Data in Python?",
    "section": "Resample to a square cell size",
    "text": "Resample to a square cell size\nThe conversion to a geographic coordinate system in the first step has created a grid with a rectangular cell size. Here is an excerpt of the output of grdinfo grav_v24.nc:\ngrav_v24.nc: x_min: 0 x_max: 360 x_inc: 0.0166666666667 name: longitude [degrees_east] n_columns: 21600\ngrav_v24.nc: y_min: -80.738 y_max: 80.738 y_inc: 0.00934467592593 name: latitude [degrees_north] n_rows: 17280\nWe can see that the increment along the x axis (x_inc) is smaller than the increment along the y axis (y_inc).\nWhile this file is still valid for analysis as it contains more information, I do need a square cell size for displaying the map in the next step. And I don’t need the full resolution, so let’s resample the grid to a 5 arc-minute resolution with GDAL:\ngdalwarp -tr 0.0833333333333 0.0833333333333 -r bilinear \"grav_v24.tif\" \"grav_v24_5min.tif\""
  },
  {
    "objectID": "legacy/how-to-load-ss-gravity-data-in-python.html#reading-the-data-in-python",
    "href": "legacy/how-to-load-ss-gravity-data-in-python.html#reading-the-data-in-python",
    "title": "How to Load Sandwell & Smith Gravity Data in Python?",
    "section": "Reading the data in Python",
    "text": "Reading the data in Python\nA simple way to access the data in our new grid is to load it in a Numpy array. There are various ways to do this. Here is an example:\nimport scipy\ndata = scipy.ndimage.imread('grav_v24_5min.tif')\nThe result is an array of shape (1938, 4320). One issue with this approach is that the geographic information about the grid is lost: things like the cell size and the location of the origin are crucial for displaying and processing the data properly."
  },
  {
    "objectID": "legacy/how-to-load-ss-gravity-data-in-python.html#display-the-data-with-interpies",
    "href": "legacy/how-to-load-ss-gravity-data-in-python.html#display-the-data-with-interpies",
    "title": "How to Load Sandwell & Smith Gravity Data in Python?",
    "section": "Display the data with interpies",
    "text": "Display the data with interpies\nUsing rasterio as a base for reading raster data, I have developed a Python module called [interpies](https://github.com/jobar8/interpies). This can be used to analyse, process and display gridded geophysical data such as gravity anomalies. The installation of interpies requires several dependencies that are detailed in the GitHub repository.\nThe main idea of interpies is that gridded data can be loaded into a grid object, which is then processed using methods that can be chained together to obtain relatively complex outputs with the minimum amount of code. Just reading and displaying the grid is done in this way:\nimport interpies\ngrid1 = interpies.open('grav_v24_5min.tif')\nax = grid1.show(figsize=(20,12), title='Global Gravity Anomalies')\n\n\n\nFree-air gravity anomalies derived from satellite altimetry (“Sandwell and Smith” data version 24.1)\n\n\nInformation about the grid can be displayed with the info method:\n&gt; grid1.info()\n* Info *\nGrid name: grav_v24_5min\nFilename: grav_v24_5min.tif\nCoordinate reference system: epsg:4326\nGrid size: 4320 columns x 1938 rows\nCell size: 0.08333\nLower left corner (pixel centre): (0.042,-80.720)\nGrid extent (outer limits): west: 0.000, east: 360.000, south: -80.762, north: 80.738\nNo Data Value: nan\nNumber of null cells: 0 (0.00%)\n\n* Statistics *\nmean = -0.2992794825345438\nsigma = 33.31866132985248\nmin = -362.504638671875\nmax = 814.0865478515625\nThere are lots of other methods available in interpies and they will be detailed in future posts."
  },
  {
    "objectID": "legacy/how-to-load-ss-gravity-data-in-python.html#references",
    "href": "legacy/how-to-load-ss-gravity-data-in-python.html#references",
    "title": "How to Load Sandwell & Smith Gravity Data in Python?",
    "section": "References",
    "text": "References\nAndersen, O. B., & Knudsen, P. (2016). Deriving the DTU15 Global high resolution marine gravity field from satellite altimetry. Abstract from ESA Living Planet Symposium 2016, Prague, Czech Republic.\nSandwell, D. T., R. D. Müller, W. H. F. Smith, E. Garcia, R. Francis (2014), New global marine gravity model from CryoSat-2 and Jason-1 reveals buried tectonic structure, Science, Vol. 346, no. 6205, pp. 65-67, doi: 10.1126/science.1258213.\nSmith, W. H. F., K. M. Marks, and T. Schmitt (2017), Airline flight paths over the unmapped ocean, Eos, 98, https://doi.org/10.1029/2017EO069127.\nSmith, W. H. F., and D. T. Sandwell (1997), Global seafloor topography from satellite altimetry and ship depth soundings, Science 277(5334), 1956–1962, https://doi.org/10.1126/science.277.5334.1956."
  }
]